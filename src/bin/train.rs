use rand::Rng;
use rayon::prelude::*;
use std::fs::{self, File}; // Th√™m fs ƒë·ªÉ qu√©t th∆∞ m·ª•c
use std::io::BufReader;
use std::sync::{Arc, RwLock};
use std::time::Duration;
use std::{env, thread};
use threes_rs::hotload_config::HotLoadConfig;
use threes_rs::{n_tuple_network::NTupleNetwork, pbt::TrainingConfig, threes_env::ThreesEnv};

// H·∫±ng s·ªë T·ª∑ l·ªá v√†ng
const GOLDEN_RATIO: f64 = 1.61803398875;

// Struct wrapper pointer (gi·ªØ nguy√™n)
struct SharedBrain {
    network: *mut NTupleNetwork,
}

#[derive(Clone, Copy, Debug, PartialEq)]
enum TrainingPolicy {
    Expectimax,
    Safe,
    Afterstate, // New policy
}

unsafe impl Send for SharedBrain {}
unsafe impl Sync for SharedBrain {}

#[derive(serde::Serialize, serde::Deserialize, Clone)]
struct StepData {
    direction: usize,
    board: [[u32; 4]; 4],
    score: f64,
}

#[derive(serde::Serialize, serde::Deserialize, Clone)]
struct GameReplay {
    score: f64,
    max_tile: u32,
    initial_board: [[u32; 4]; 4],
    steps: Vec<StepData>,
}

fn main() {
    let num_threads = 8;
    let gamma = 0.995;
    let args: Vec<String> = env::args().collect();

    // --- LOGIC 1: T·ª∞ ƒê·ªòNG T√åM FILE SAVE M·ªöI NH·∫§T (AUTO-DISCOVERY) ---
    // N·∫øu ng∆∞·ªùi d√πng kh√¥ng nh·∫≠p s·ªë, t·ª± ƒë·ªông qu√©t th∆∞ m·ª•c t√¨m file msgpack c√≥ s·ªë to nh·∫•t.
    let override_episode = find_latest_checkpoint().unwrap_or(0);

    println!("üîé Start Episode: {}", override_episode);

    // Policy
    let policy_arg = if args.len() > 2 {
        args[2].to_lowercase()
    } else {
        "expect".to_string()
    };

    let training_policy = match policy_arg.as_str() {
        "expect" => {
            println!("üß† Training Mode: EXPECTIMAX");
            TrainingPolicy::Expectimax
        }
        "safe" => {
            println!("üß† Training Mode: SAFE");
            TrainingPolicy::Safe
        }
        _ => {
            println!("üß† Training Mode: AFTERSTATE (Default/Optimized)");
            TrainingPolicy::Afterstate
        }
    };

    let multiplier = args[2].to_lowercase();

    let mut buff_multiplier = 1.0;

    println!("Multiplier Strategy: {}", multiplier);

    // --- SETUP BRAIN ---
    let mut brain = if override_episode > 0 {
        let filename = format!("brain_ep_{}.msgpack", override_episode);
        println!("üìÇ Loading brain: {}", filename);
        let b = NTupleNetwork::load_from_msgpack(&filename)
            .expect("‚ùå Kh√¥ng t√¨m th·∫•y file checkpoint!");
        println!(
            "üßê LOAD DATA: E={:.1}, S={:.1}, M={:.1}, D={:.1}",
            b.w_empty, b.w_snake, b.w_merge, b.w_disorder
        );
        b
    } else {
        println!("‚ú® T·∫°o n√£o m·ªõi tinh (Episode 0)...");
        NTupleNetwork::new(0.1, gamma)
    };

    // Logic t∆∞∆°ng th√≠ch ng∆∞·ª£c cho file c≈©
    if override_episode > 0 && brain.total_episodes == 0 {
        println!(
            "‚ö†Ô∏è File c≈© ch∆∞a c√≥ total_episodes, c·∫≠p nh·∫≠t th·ªß c√¥ng th√†nh {}",
            override_episode
        );
        brain.total_episodes = override_episode;
    }

    // Safety checks
    // if brain.w_empty == 0.0 {
    //     brain.w_empty = 50.0;
    // }
    // if brain.w_snake == 0.0 {
    //     brain.w_snake = 50.0;
    // }
    // if brain.w_merge == 0.0 {
    //     brain.w_merge = 50.0;
    // }
    // if brain.w_disorder == 0.0 {
    //     brain.w_disorder = 50.0;
    // }

    // Config Watcher & PBT
    let hot_config = Arc::new(RwLock::new(HotLoadConfig::default()));
    start_config_watcher(hot_config.clone());
    println!("üî• Hot Reload ENABLED - ƒêang theo d√µi config.json");
    // let pbt_manager = Arc::new(Mutex::new(PBTManager::new()));

    let chunk_episodes = 80_000;
    let total_target_episodes = 100_000_000;

    // --- CHECKPOINT G·ªêC (SINGLE SOURCE OF TRUTH) ---
    // ƒê√¢y l√† b·∫£n chu·∫©n. M·ªçi v√≤ng l·∫∑p ƒë·ªÅu clone t·ª´ ƒë√¢y ra.
    let mut best_stable_brain = brain.clone();

    // K·ª∑ l·ª•c ƒë∆∞·ª£c t√≠nh d·ª±a tr√™n ƒëi·ªÉm EVAL (evaluation training), kh√¥ng ph·∫£i ƒëi·ªÉm train noisy
    let mut best_eval_avg = best_stable_brain.best_overall_avg;

    println!(
        "üöÄ Start Training. Baseline Eval Record: {:.2}",
        best_eval_avg
    );
    println!(
        "üìä Current Checkpoint: Ep {} | Config: E={:.1} S={:.1} M={:.1} D={:.1}",
        best_stable_brain.total_episodes,
        best_stable_brain.w_empty,
        best_stable_brain.w_snake,
        best_stable_brain.w_merge,
        best_stable_brain.w_disorder
    );

    // S·ªë l∆∞·ª£ng game evaluation. 50k ƒë·ªÉ ƒë√°nh gi√° k·ªπ.
    let eval_games = chunk_episodes / 10;

    loop {
        let loop_start = std::time::Instant::now();

        // ============================================================
        // B∆Ø·ªöC 0: RESET V·ªÄ B·∫¢N CHU·∫®N ƒê·ªÇ TRAIN
        // ============================================================
        brain = best_stable_brain.clone();

        // T·∫°o pointer M·ªöI cho v√≤ng l·∫∑p n√†y (Quan tr·ªçng!)
        // let brain_ptr = SharedBrain {
        //     network: &mut brain as *mut NTupleNetwork,
        // };
        // let shared_brain_loop = Arc::new(brain_ptr);

        // ------------------------------------------------------
        // 1. LOGIC BUFF (Random 1 ch·ªâ s·ªë)
        // ------------------------------------------------------
        // let rng = rand::rng();
        // let buff_idx = rng.random_range(0..4);

        // match buff_idx {
        //     0 => {
        //         brain.w_empty *= buff_multiplier;
        //         print!("‚ú® BUFF EMPTY! ");
        //     }
        //     1 => {
        //         brain.w_snake *= buff_multiplier;
        //         print!("üêç BUFF SNAKE! ");
        //     }
        //     2 => {
        //         brain.w_merge *= buff_multiplier;
        //         print!("üîó BUFF MERGE! ");
        //     }
        //     _ => {
        //         brain.w_disorder *= buff_multiplier;
        //         print!("‚ö° BUFF DISORDER! ");
        //     }
        // }

        println!(
            "-> Test Config: {:.1}/{:.1}/{:.1}/{:.1}",
            brain.w_empty, brain.w_snake, brain.w_merge, brain.w_disorder
        );

        // ƒêi·ªÅu ch·ªânh Phase d·ª±a tr√™n ng∆∞·ª°ng
        if brain.w_empty > 10000.0
            || brain.w_snake > 10000.0
            || brain.w_merge > 10000.0
            || brain.w_disorder > 10000.0
        {
            brain.phase = false; // Chuy·ªÉn sang gi·∫£m
        }

        // if brain.w_empty < 60.0
        //     || brain.w_snake < 60.0
        //     || brain.w_merge < 60.0
        //     || brain.w_disorder < 60.0
        // {
        //     brain.phase = true; // Chuy·ªÉn sang tƒÉng
        // }

        if brain.phase {
            buff_multiplier = GOLDEN_RATIO;
            print!(" (PHASE: TƒÇNG üìà) ");
        } else {
            buff_multiplier = 1.0 / GOLDEN_RATIO;
            print!(" (PHASE: GI·∫¢M üìâ) ");
        }

        println!("-> Buff Multiplier: {:.2}", buff_multiplier);

        // ============================================================
        // B∆Ø·ªöC 1: PARALLEL TRAINING - M·ªói thread clone brain ri√™ng
        // M·ª•c ƒë√≠ch: T√¨m CONFIG t·ªëi ∆∞u cho brain hi·ªán t·∫°i
        // ============================================================
        println!(
            "üèãÔ∏è Training Phase ({} games, {} threads)...",
            chunk_episodes, num_threads
        );

        let ep_per_thread = chunk_episodes as u32 / num_threads;
        let current_base_ep = best_stable_brain.total_episodes;

        // Clone brain g·ªëc ƒë·ªÉ share (read-only reference)
        let base_brain = best_stable_brain.clone();
        let base_config = TrainingConfig {
            w_empty: brain.w_empty,
            w_snake: brain.w_snake,
            w_merge: brain.w_merge,
            w_disorder: brain.w_disorder,
        };

        // Clone hot_config ƒë·ªÉ share v√†o c√°c thread
        let hot_config_clone = hot_config.clone();

        // M·ªói thread tr·∫£ v·ªÅ (avg_score, trained_brain, config)
        let thread_results: Vec<(f64, NTupleNetwork, TrainingConfig)> = (0..num_threads)
            .into_par_iter()
            .map(|t_id| {
                // Clone brain RI√äNG cho thread n√†y
                let mut local_brain = base_brain.clone();
                let mut local_env = ThreesEnv::new(gamma);
                let mut rng = rand::rng();

                // T·∫°o config ri√™ng cho thread n√†y (mutate t·ª´ base)
                let mut thread_config = base_config;

                // M·ªói thread mutate ng·∫´u nhi√™n 1-2 weights
                for _ in 0..2 {
                    let param_idx = rng.random_range(0..4);
                    let mutate_factor = rng.random_range(1..6) as f64;
                    // tinh buff dua vao buff_multiplier
                    let mutate = buff_multiplier.powf(mutate_factor);

                    match param_idx {
                        0 => thread_config.w_empty *= mutate,
                        1 => thread_config.w_snake *= mutate,
                        2 => thread_config.w_merge *= mutate,
                        _ => thread_config.w_disorder *= mutate,
                    }
                }

                local_brain.w_empty = thread_config.w_empty;
                local_brain.w_snake = thread_config.w_snake;
                local_brain.w_merge = thread_config.w_merge;
                local_brain.w_disorder = thread_config.w_disorder;

                // Train ri√™ng bi·ªát
                let mut total_score = 0.0;
                for local_ep in 0..ep_per_thread {
                    let current_global_ep = (local_ep * num_threads + t_id) + current_base_ep;
                    let progress = current_global_ep as f64 / total_target_episodes as f64;

                    // ============ HOT RELOAD - ƒê·ªåC CONFIG.JSON ============
                    let current_hot = *hot_config_clone.read().unwrap();
                    let mut effective_config = thread_config;

                    if let Some(v) = current_hot.w_empty_override {
                        effective_config.w_empty = v;
                    }
                    if let Some(v) = current_hot.w_snake_override {
                        effective_config.w_snake = v;
                    }
                    if let Some(v) = current_hot.w_merge_override {
                        effective_config.w_merge = v;
                    }
                    if let Some(v) = current_hot.w_disorder_override {
                        effective_config.w_disorder = v;
                    }

                    local_env.set_config(effective_config);

                    // üî• QUAN TR·ªåNG: C·∫≠p nh·∫≠t ng∆∞·ª£c l·∫°i ƒë·ªÉ b√°o c√°o cu·ªëi thread v√† Best Config mang s·ªë n√†y!
                    thread_config = effective_config;
                    // ========================================================

                    let mut current_alpha = (0.01 * (1.0 - progress)).max(0.0001);
                    if let Some(v) = current_hot.alpha_override {
                        current_alpha = v;
                    }
                    let mut current_epsilon = (0.2 * (1.0 - (progress / 0.8))).max(0.01);
                    if let Some(v) = current_hot.epsilon_override {
                        current_epsilon = v;
                    }

                    local_env.reset();
                    // local_brain.reset_traces(); // Removed: Traces now managed by env
                    let mut step_count = 0;
                    while !local_env.game.game_over {
                        step_count += 1;
                        if step_count > 20000 {
                            break;
                        }

                        let action = if rng.random_bool(current_epsilon.into()) {
                            local_env.get_random_valid_action()
                        } else {
                            match training_policy {
                                TrainingPolicy::Expectimax => {
                                    local_env.get_best_action_expectimax(&mut local_brain)
                                }
                                TrainingPolicy::Safe => {
                                    local_env.get_best_action_safe(&mut local_brain)
                                }
                                TrainingPolicy::Afterstate => {
                                    local_env.get_best_action_afterstate(&mut local_brain)
                                }
                            }
                        };

                        local_env.train_step(&mut local_brain, action, current_alpha);
                    }

                    total_score += local_env.game.score as f64;

                    // Log progress (ch·ªâ thread 0)
                    if t_id == 0 && local_ep % 2000 == 0 {
                        let running_avg = total_score / (local_ep + 1) as f64;
                        print!(
                            "\r   T0: {:>5}/{} | Avg: {:>5.0} | Cfg: S{:.0} M{:.0}   ",
                            local_ep,
                            ep_per_thread,
                            running_avg,
                            effective_config.w_snake,
                            effective_config.w_merge
                        );
                        use std::io::Write;
                        std::io::stdout().flush().unwrap();
                    }
                }

                let avg_score = total_score / ep_per_thread as f64;
                (avg_score, local_brain, thread_config)
            })
            .collect();

        println!(); // Newline sau progress

        // T√¨m best config v√† merge weights
        let mut best_score = 0.0f64;
        let mut best_config = base_config;

        println!("   Thread Results:");
        for (i, (score, _, cfg)) in thread_results.iter().enumerate() {
            println!(
                "   T{}: Avg={:.0} | E={:.0} S={:.0} M={:.0} D={:.0}",
                i, score, cfg.w_empty, cfg.w_snake, cfg.w_merge, cfg.w_disorder
            );
            if *score > best_score {
                best_score = *score;
                best_config = *cfg;
            }
        }

        // Merge weights b·∫±ng Softmax Weighted Averaging (Chuy√™n nghi·ªáp h∆°n Linear)
        // Weight_i = exp(Score_i / T) / Sum(exp(Score_j / T))
        // T (Temperature) cao -> Merge ƒë·ªÅu. T th·∫•p -> Ch·ªçn l·ªçc ng∆∞·ªùi gi·ªèi nh·∫•t.
        let max_score = thread_results
            .iter()
            .map(|(s, _, _)| *s)
            .fold(0.0, f64::max);
        let temperature = max_score * 0.1; // T = 10% c·ªßa max score

        // T√≠nh m·∫´u s·ªë (denominator) ·ªïn ƒë·ªãnh s·ªë h·ªçc (tr·ª´ max_score ƒë·ªÉ tr√°nh overflow exp)
        let mut sum_exp = 0.0;
        let mut softmax_weights = Vec::new();

        for (score, _, _) in &thread_results {
            let val = ((score - max_score) / temperature).exp();
            sum_exp += val;
            softmax_weights.push(val);
        }

        // Normalize weights
        for w in &mut softmax_weights {
            *w /= sum_exp;
        }

        println!(
            "   -> Softmax Merge Weights: {:?}",
            softmax_weights
                .iter()
                .map(|w| (w * 100.0).round() as i32)
                .collect::<Vec<_>>()
        );

        // Reset brain weights v·ªÅ 0 tr∆∞·ªõc khi merge
        // L∆∞u √Ω: weights n·∫±m ·ªü c·∫•p Network, kh√¥ng ph·∫£i trong TupleConfig
        for w_table in brain.weights.iter_mut() {
            for val in w_table.iter_mut() {
                *val = 0.0;
            }
        }

        // Softmax Weighted Merge
        for (idx, (_, trained_brain, _)) in thread_results.iter().enumerate() {
            let weight = softmax_weights[idx];

            // Duy·ªát qua t·ª´ng b·∫£ng weights (Master Weights)
            for (table_idx, w_table) in trained_brain.weights.iter().enumerate() {
                // C·ªông d·ªìn v√†o b·∫£ng t∆∞∆°ng ·ª©ng c·ªßa brain ch√≠nh
                for (w_idx, val) in w_table.iter().enumerate() {
                    brain.weights[table_idx][w_idx] += val * weight;
                }
            }
        }

        let train_avg = best_score;
        println!(
            "   -> Best Config: E={:.3} S={:.3} M={:.3} D={:.3} (Avg: {:.0})",
            best_config.w_empty,
            best_config.w_snake,
            best_config.w_merge,
            best_config.w_disorder,
            train_avg
        );

        // ============================================================
        // B∆Ø·ªöC 2: SELECT CONFIG PHASE (ƒê√£ t√≠ch h·ª£p ·ªü tr√™n)
        // best_config ƒë√£ ƒë∆∞·ª£c t√¨m ra t·ª´ Thread Results
        // ============================================================

        // In ra config th·ª±c t·∫ø s·∫Ω d√πng cho Eval (ƒë√£ t√≠nh ƒë·∫øn Hot Reload n·∫øu c√≥)
        let current_hot_val = *hot_config.read().unwrap();
        let mut actual_eval_config = best_config;
        if let Some(v) = current_hot_val.w_empty_override {
            actual_eval_config.w_empty = v;
        }
        if let Some(v) = current_hot_val.w_snake_override {
            actual_eval_config.w_snake = v;
        }
        if let Some(v) = current_hot_val.w_merge_override {
            actual_eval_config.w_merge = v;
        }
        if let Some(v) = current_hot_val.w_disorder_override {
            actual_eval_config.w_disorder = v;
        }

        println!(
            "üìä Evaluation Training ({} games) with Best Config...",
            eval_games
        );
        println!(
            "   Cfg Th·ª±c T·∫ø: Empty={:.1} Snake={:.1} Merge={:.1} Disorder={:.1}",
            actual_eval_config.w_empty,
            actual_eval_config.w_snake,
            actual_eval_config.w_merge,
            actual_eval_config.w_disorder
        );

        // Clone model ·ªïn ƒë·ªãnh ƒë·ªÉ train evaluation
        // let eval_brain = best_stable_brain.clone();

        // Best config ƒë√£ in ·ªü tr√™n

        // Clone MERGED BRAIN ƒë·ªÉ train evaluation
        // ƒê√¢y l√† ƒëi·ªÉm quan tr·ªçng: T·∫≠n d·ª•ng ki·∫øn th·ª©c ƒë√£ h·ªçc v√† merge t·ª´ 100k games tr∆∞·ªõc
        let mut eval_brain = brain.clone();

        // G√°n config t·ªëi ∆∞u v√†o eval_brain
        eval_brain.w_empty = best_config.w_empty;
        eval_brain.w_snake = best_config.w_snake;
        eval_brain.w_merge = best_config.w_merge;
        eval_brain.w_disorder = best_config.w_disorder;

        // Train th·∫≠t 80k games v·ªõi config m·ªõi tr√™n n·ªÅn merged brain
        // Truy·ªÅn hot_config ƒë·ªÉ c√≥ th·ªÉ override b·∫•t c·ª© l√∫c n√†o!
        let (eval_avg, eval_max, trained_eval_brain, best_replay_opt) = run_evaluation_training(
            eval_brain,
            best_config,
            eval_games,
            num_threads,
            gamma,
            total_target_episodes,
            best_stable_brain.total_episodes + chunk_episodes, // Offset ƒë√£ tƒÉng l√™n
            training_policy,
            hot_config.clone(), // üî• TRUY·ªÄN HOT CONFIG V√ÄO!
        );

        // L∆∞u replay t·ªët nh·∫•t n·∫øu c√≥
        if let Some(replay) = best_replay_opt {
            let replay_json = serde_json::to_string(&replay).unwrap();
            if let Err(e) = std::fs::write("best_replay.json", replay_json) {
                eprintln!("‚ö†Ô∏è Failed to save replay: {}", e);
            } else {
                println!(
                    "üé¨ Saved Best Replay of this iteration (Score: {:.0}) to best_replay.json",
                    replay.score
                );
            }
        }

        let duration = loop_start.elapsed();
        println!(
            "   -> üìä Eval Result: Avg = {:.2} (Max: {:.0}) | Record: {:.2}",
            eval_avg, eval_max, best_eval_avg
        );

        // ============================================================
        // B∆Ø·ªöC 4: SO S√ÅNH & SAVE
        // Ch·ªâ save n·∫øu ƒëi·ªÉm Eval cao h∆°n k·ª∑ l·ª•c c≈©
        // ============================================================
        if eval_avg > best_eval_avg {
            println!("‚úÖ NEW RECORD! ({:.2} > {:.2})", eval_avg, best_eval_avg);

            // C·∫≠p nh·∫≠t k·ª∑ l·ª•c
            best_eval_avg = eval_avg;

            // C·∫≠p nh·∫≠t th√¥ng s·ªë v√†o Brain ƒë√£ train ƒë·ªÉ l∆∞u
            // T√çNH TO√ÄN B·ªò: 100k h·ªçc b·∫©n + 50k h·ªçc th·∫≠t ƒë·ªÅu ƒë∆∞·ª£c ghi nh·∫≠n
            let mut save_brain = trained_eval_brain;
            save_brain.total_episodes =
                best_stable_brain.total_episodes + chunk_episodes + eval_games;
            save_brain.best_overall_avg = eval_avg;

            // Config ƒë√£ ƒë∆∞·ª£c g√°n tr∆∞·ªõc khi train n√™n kh√¥ng c·∫ßn g√°n l·∫°i

            // Save Checkpoint
            best_stable_brain = save_brain.clone(); // C·∫≠p nh·∫≠t m·ªëc neo
            let filename = format!("brain_ep_{}.msgpack", save_brain.total_episodes);
            if let Err(e) = save_brain.export_to_msgpack(&filename) {
                eprintln!("‚ùå Save Error: {}", e);
            } else {
                println!("üíæ Saved checkpoint: {}", filename);
            }
        } else {
            println!(
                "‚ùå REJECTED. (Eval {:.2} <= Record {:.2})",
                eval_avg, best_eval_avg
            );
            println!("üîÑ Discarding changes. Reverting to previous best.");
            // Kh√¥ng l√†m g√¨ c·∫£, v√≤ng l·∫∑p sau s·∫Ω t·ª± clone l·∫°i t·ª´ best_stable_brain c≈©
        }

        println!(
            "‚è±Ô∏è Loop Time: {:.1}s\n-----------------------------------",
            duration.as_secs_f64()
        );
    }
}

fn find_latest_checkpoint() -> Option<u32> {
    let mut max_ep = 0;
    let mut found = false;

    if let Ok(entries) = fs::read_dir(".") {
        for entry in entries.flatten() {
            let path = entry.path();
            if let Some(name) = path.file_name().and_then(|n| n.to_str()) {
                // Ki·ªÉm tra xem c√≥ ƒë√∫ng ƒë·ªãnh d·∫°ng file kh√¥ng
                if name.starts_with("brain_ep_") && name.ends_with(".msgpack") {
                    let num_part = name
                        .trim_start_matches("brain_ep_")
                        .trim_end_matches(".msgpack");

                    if let Ok(ep) = num_part.parse::<u32>() {
                        println!("  üîç Found: {} (Ep: {})", name, ep); // Log ƒë·ªÉ b√°c th·∫•y n√≥ t√¨m ƒë∆∞·ª£c g√¨
                        if ep >= max_ep {
                            max_ep = ep;
                            found = true;
                        }
                    }
                }
            }
        }
    }

    if found {
        println!("‚úÖ Auto-discovered latest checkpoint: Ep {}", max_ep);
        Some(max_ep)
    } else {
        println!("‚ö†Ô∏è No checkpoints found in current directory.");
        None
    }
}

// ... (C√°c h√†m kh√°c gi·ªØ nguy√™n: start_config_watcher, run_training_parallel) ...
// Nh·ªõ copy n·ªët h√†m run_training_parallel ·ªü code tr∆∞·ªõc v√†o nh√©!
fn start_config_watcher(shared_hot_config: Arc<RwLock<HotLoadConfig>>) {
    thread::spawn(move || {
        let mut last_cfg = HotLoadConfig::default();
        loop {
            thread::sleep(Duration::from_secs(2));
            if let Ok(file) = File::open("config.json") {
                let reader = BufReader::new(file);
                match serde_json::from_reader::<_, HotLoadConfig>(reader) {
                    Ok(new_cfg) => {
                        let mut changed = false;
                        // Ki·ªÉm tra thay ƒë·ªïi weights
                        if new_cfg.w_empty_override != last_cfg.w_empty_override
                            || new_cfg.w_snake_override != last_cfg.w_snake_override
                            || new_cfg.w_merge_override != last_cfg.w_merge_override
                            || new_cfg.w_disorder_override != last_cfg.w_disorder_override
                        {
                            changed = true;
                        }

                        // ‚úÖ S·ª¨A: So s√°nh ƒë√∫ng t√™n bi·∫øn alpha_override
                        if new_cfg.alpha_override != last_cfg.alpha_override {
                            changed = true;
                        }
                        // ‚úÖ S·ª¨A: Th√™m check epsilon thay ƒë·ªïi
                        if new_cfg.epsilon_override != last_cfg.epsilon_override {
                            changed = true;
                        }

                        if changed {
                            print!("\nüî• HOT RELOAD (Overrides): ");
                            if let Some(v) = new_cfg.w_empty_override {
                                print!("Empty={:.1} ", v);
                            }
                            if let Some(v) = new_cfg.w_snake_override {
                                print!("Snake={:.1} ", v);
                            }
                            if let Some(v) = new_cfg.w_merge_override {
                                print!("Merge={:.1} ", v);
                            }
                            if let Some(v) = new_cfg.w_disorder_override {
                                print!("DisOrder={:.1} ", v);
                            }

                            // ‚úÖ S·ª¨A: In ƒë√∫ng t√™n bi·∫øn
                            if let Some(v) = new_cfg.alpha_override {
                                print!("Œ±={:.4} ", v);
                            }
                            if let Some(v) = new_cfg.epsilon_override {
                                print!("Œµ={:.4} ", v);
                            }
                            if let Some(v) = new_cfg.eval_epsilon_override {
                                print!("Ev_Œµ={:.4} ", v);
                            }

                            println!();
                            last_cfg = new_cfg.clone();
                        }
                        let mut write_guard = shared_hot_config.write().unwrap();
                        *write_guard = new_cfg;
                    }
                    Err(_e) => {}
                }
            }
        }
    });
}

/// H√†m Evaluation Training: TRAIN TH·∫¨T v·ªõi config c·ªë ƒë·ªãnh
/// Kh√°c v·ªõi run_training_parallel:
/// - Kh√¥ng d√πng PBT evolve (config c·ªë ƒë·ªãnh)
/// - Kh√¥ng c√≥ Hot Reload
/// - Tr·∫£ v·ªÅ brain ƒë√£ train ƒë·ªÉ c√≥ th·ªÉ save
fn run_evaluation_training(
    mut brain: NTupleNetwork, // Ownership - s·∫Ω ƒë∆∞·ª£c train v√† tr·∫£ v·ªÅ
    config: TrainingConfig,   // Config c∆° s·ªü ƒë·ªÉ train
    total_games: u32,         // S·ªë l∆∞·ª£ng game (vd: 80,000)
    num_threads: u32,
    gamma: f64,
    total_target_episodes: u32,
    start_offset: u32,
    policy: TrainingPolicy,
    hot_config: Arc<RwLock<HotLoadConfig>>, // üî• HOT CONFIG ƒê√É TH√äM!
) -> (f64, f64, NTupleNetwork, Option<GameReplay>) {
    // Tr·∫£ v·ªÅ (Avg Score, Max Score, Trained Brain, Best Replay)

    // T·∫°o shared pointer ƒë·ªÉ c√°c thread c√πng update weights
    let brain_ptr = SharedBrain {
        network: &mut brain as *mut NTupleNetwork,
    };
    let shared_brain = Arc::new(brain_ptr);

    let ep_per_thread = total_games / num_threads;

    // Ch·∫°y song song - m·ªói thread train m·ªôt ph·∫ßn games
    // Return: (local_scores, local_best_replay)
    let results: Vec<(Vec<f64>, Option<GameReplay>)> = (0..num_threads)
        .into_par_iter()
        .map(|t_id| {
            let mut local_env = ThreesEnv::new(gamma);

            let ptr = shared_brain.network;
            let local_brain = unsafe { &mut *ptr };

            let mut local_scores = Vec::with_capacity(ep_per_thread as usize);
            let mut rng = rand::rng();

            let mut local_best_replay: Option<GameReplay> = None;
            let mut local_max_score = 0.0;

            for local_ep in 0..ep_per_thread {
                let current_global_ep = (local_ep * num_threads + t_id) + start_offset;
                let progress = current_global_ep as f64 / total_target_episodes as f64;

                // ============ HOT RELOAD - ƒê·ªåC CONFIG.JSON ============
                let current_hot = *hot_config.read().unwrap();
                let mut effective_config = config; // B·∫Øt ƒë·∫ßu t·ª´ config c∆° s·ªü

                if let Some(v) = current_hot.w_empty_override {
                    effective_config.w_empty = v;
                }
                if let Some(v) = current_hot.w_snake_override {
                    effective_config.w_snake = v;
                }
                if let Some(v) = current_hot.w_merge_override {
                    effective_config.w_merge = v;
                }
                if let Some(v) = current_hot.w_disorder_override {
                    effective_config.w_disorder = v;
                }

                local_env.set_config(effective_config);

                // C·∫≠p nh·∫≠t th√¥ng s·ªë v√†o brain ƒë·ªÉ khi Save n√≥ mang th√¥ng s·ªë m·ªõi n√†y!
                local_brain.w_empty = effective_config.w_empty;
                local_brain.w_snake = effective_config.w_snake;
                local_brain.w_merge = effective_config.w_merge;
                local_brain.w_disorder = effective_config.w_disorder;
                // ========================================================

                // Alpha & Epsilon decay
                let mut current_alpha = (0.01 * (1.0 - progress)).max(0.0001);
                if let Some(v) = current_hot.alpha_override {
                    current_alpha = v;
                }
                // üî• D√πng eval_epsilon t·ª´ config n·∫øu c√≥, kh√¥ng th√¨ d√πng epsilon decay
                let current_epsilon = current_hot
                    .eval_epsilon_override
                    .unwrap_or_else(|| (0.2 * (1.0 - (progress / 0.8))).max(0.01));

                // GAME LOOP
                local_env.reset();
                // local_brain.reset_traces(); // Removed: Traces now managed by env

                // TRACKING FOR REPLAY
                let mut current_steps = Vec::new();
                let initial_board_state = {
                    let mut arr = [[0u32; 4]; 4];
                    for r in 0..4 {
                        for c in 0..4 {
                            arr[r][c] = local_env.game.board[r][c].value;
                        }
                    }
                    arr
                };

                let mut step_count = 0;
                while !local_env.game.game_over {
                    step_count += 1;
                    if step_count > 20000 {
                        break;
                    }

                    let action = if rng.random_bool(current_epsilon.into()) {
                        local_env.get_random_valid_action()
                    } else {
                        local_env.get_best_action_expectimax(local_brain)
                    };

                    // Execute Step
                    local_env.train_step(local_brain, action, current_alpha);

                    // Record Step
                    let board_snap = {
                        let mut arr = [[0u32; 4]; 4];
                        for r in 0..4 {
                            for c in 0..4 {
                                arr[r][c] = local_env.game.board[r][c].value;
                            }
                        }
                        arr
                    };

                    current_steps.push(StepData {
                        direction: action as usize,
                        board: board_snap,
                        score: local_env.game.score,
                    });
                }

                let game_score = local_env.game.score as f64;
                local_scores.push(game_score);

                // Update Request Best Replay
                if game_score > local_max_score {
                    local_max_score = game_score;
                    local_best_replay = Some(GameReplay {
                        score: game_score,
                        max_tile: local_env.game.get_highest_tile_value(),
                        initial_board: initial_board_state,
                        steps: current_steps,
                    });
                }

                // Log progress (ch·ªâ thread 0)
                if t_id == 0 && local_ep % 1000 == 0 {
                    print!(
                        "\r   Eval: {:>6}/{} | Last: {:>5.0} | HotCfg: S{:.0} M{:.0}   ",
                        local_ep * num_threads,
                        total_games,
                        local_env.game.score,
                        effective_config.w_snake,
                        effective_config.w_merge
                    );
                    use std::io::Write;
                    std::io::stdout().flush().unwrap();
                }
            }

            (local_scores, local_best_replay)
        })
        .collect();

    println!(); // Newline sau progress

    let all_scores: Vec<f64> = results.iter().map(|(s, _)| s).flatten().cloned().collect();
    let avg = all_scores.iter().sum::<f64>() / all_scores.len() as f64;
    let max = all_scores.iter().fold(0.0f64, |a, &b| a.max(b));

    // Find global best replay
    let mut global_best_replay: Option<GameReplay> = None;
    let mut global_max_score = 0.0;

    for (_, replay_opt) in results {
        if let Some(replay) = replay_opt {
            if replay.score > global_max_score {
                global_max_score = replay.score;
                global_best_replay = Some(replay);
            }
        }
    }

    // T√≠nh ƒëi·ªÉm trung b√¨nh c·ªßa 10% th·∫•p nh·∫•t (Bottom 10%)
    let mut sorted_scores = all_scores.clone();
    sorted_scores.sort_by(|a, b| a.partial_cmp(b).unwrap());
    let bot10_count = (sorted_scores.len() as f64 * 0.1).ceil() as usize;
    let bot10_avg: f64 = sorted_scores.iter().take(bot10_count).sum::<f64>() / bot10_count as f64;

    println!(
        "   üìâ Bottom 10% Avg: {:.2} ({} games)",
        bot10_avg, bot10_count
    );

    (avg, max, brain, global_best_replay)
}
