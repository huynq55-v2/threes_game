use crate::deck_tracker::DeckTracker;
use crate::game::{Direction, Game};
use crate::n_tuple_network::NTupleNetwork;
use crate::pbt::TrainingConfig;
use rand::Rng as _;

pub struct ThreesEnv {
    pub game: Game,
    gamma: f64,
    // adaptive_manager: AdaptiveManager, // Unused
    pub config: TrainingConfig,

    // TH√äM 2 tr∆∞·ªùng n√†y v√†o Env (Local State)
    pub traces: Vec<Vec<f64>>,
    pub active_trace_indices: Vec<Vec<usize>>,
}

impl ThreesEnv {
    pub fn new(gamma: f64) -> Self {
        let game = Game::new();
        ThreesEnv {
            game,
            gamma: gamma,
            // adaptive_manager: AdaptiveManager::new(),
            config: TrainingConfig::default(),

            // Kh·ªüi t·∫°o r·ªóng
            traces: Vec::new(),
            active_trace_indices: Vec::new(),
        }
    }

    // H√†m m·ªõi ƒë·ªÉ kh·ªüi t·∫°o/reset traces
    pub fn reset_traces(&mut self, num_tables: usize, table_sizes: &[usize]) {
        // N·∫øu ch∆∞a kh·ªüi t·∫°o ho·∫∑c k√≠ch th∆∞·ªõc sai -> C·∫•p ph√°t l·∫°i
        if self.traces.len() != num_tables {
            self.traces.clear();
            self.active_trace_indices.clear();
            for &size in table_sizes {
                self.traces.push(vec![0.0; size]);
                self.active_trace_indices.push(Vec::with_capacity(1000));
            }
        } else {
            // N·∫øu ƒë√£ c√≥, ch·ªâ c·∫ßn clear gi√° tr·ªã active
            for (table_idx, indices) in self.active_trace_indices.iter_mut().enumerate() {
                for &feat_idx in indices.iter() {
                    self.traces[table_idx][feat_idx] = 0.0;
                }
                indices.clear();
            }
        }
    }

    pub fn set_config(&mut self, new_cfg: TrainingConfig) {
        self.config = new_cfg;
    }

    pub fn set_gamma(&mut self, gamma: f64) {
        self.gamma = gamma;
    }

    // Return signature: (Board, Reward, Done, Hints)
    fn step(&mut self, action: u32) -> (Vec<u32>, f64, bool, Vec<u32>) {
        let dir = match action {
            0 => Direction::Up,
            1 => Direction::Down,
            2 => Direction::Left,
            3 => Direction::Right,
            _ => {
                return (
                    self.get_board_flat().to_vec(),
                    -10.0,
                    true,
                    self.game.hints.clone(),
                )
            }
        };

        // let phi_old = get_composite_potential(&self.game.board, &self.config);
        let score_old = self.game.score;

        if self.game.can_move(dir) {
            let (_, _) = self.game.move_dir(dir);

            let game_over = self.game.check_game_over();

            // let phi_new = get_composite_potential(&self.game.board, &self.config);
            let score_new = self.game.score;

            let base_reward = (score_new - score_old) as f64;
            // let gamma = self.gamma;
            // let raw_shaping = (gamma * phi_new) - phi_old;

            // let final_reward = self
            //     .adaptive_manager
            //     .update_and_scale(base_reward, raw_shaping);

            let final_reward = base_reward; // no shaping

            (
                self.get_board_flat().to_vec(),
                final_reward,
                game_over,
                self.game.hints.clone(),
            )
        } else {
            unreachable!()
        }
    }

    fn valid_moves(&self) -> Vec<bool> {
        vec![
            self.game.can_move(Direction::Up),
            self.game.can_move(Direction::Down),
            self.game.can_move(Direction::Left),
            self.game.can_move(Direction::Right),
        ]
    }

    pub fn get_random_valid_action(&self) -> u32 {
        let mut valid_actions = Vec::new();

        // 1. Duy·ªát qua 4 h∆∞·ªõng ƒë·ªÉ thu th·∫≠p c√°c h∆∞·ªõng ƒëi ƒë∆∞·ª£c
        for action in 0..4 {
            let dir = match action {
                0 => Direction::Up,
                1 => Direction::Down,
                2 => Direction::Left,
                3 => Direction::Right,
                _ => continue,
            };

            // D√πng h√†m can_move m√† ch√∫ng ta ƒë√£ th·ªëng nh·∫•t ƒë·ªÉ ki·ªÉm tra
            if self.game.can_move(dir) {
                valid_actions.push(action);
            }
        }

        // 2. N·∫øu danh s√°ch tr·ªëng -> H·∫øt ƒë∆∞·ªùng ƒëi
        if valid_actions.is_empty() {
            return 100; // Sentinel value b√°o hi·ªáu Game Over
        }

        // 3. Ch·ªçn ng·∫´u nhi√™n m·ªôt action trong danh s√°ch h·ª£p l·ªá
        let mut rng = rand::rng();
        // D√πng choose ƒë·ªÉ b·ªëc m·ªôt ph·∫ßn t·ª≠ ng·∫´u nhi√™n (An to√†n h∆°n loop random)
        use rand::prelude::IndexedRandom;
        *valid_actions.choose(&mut rng).unwrap() as u32
    }

    pub fn get_best_action_safe(&self, brain: &NTupleNetwork) -> u32 {
        let mut best_action = 0;
        let mut max_of_min_quality = f64::NEG_INFINITY;

        for action_idx in 0..4 {
            let dir = match action_idx {
                0 => Direction::Up,
                1 => Direction::Down,
                2 => Direction::Left,
                3 => Direction::Right,
                _ => continue,
            };

            let outcomes = self.game.get_all_possible_outcomes(dir);

            if !outcomes.is_empty() {
                let mut min_quality = f64::INFINITY;

                for outcome in &outcomes {
                    let future_value = brain.predict_game(outcome);
                    let move_quality = future_value;

                    if move_quality < min_quality {
                        min_quality = move_quality;
                    }
                }

                if min_quality > max_of_min_quality {
                    max_of_min_quality = min_quality;
                    best_action = action_idx;
                }
            }
        }

        best_action as u32
    }

    pub fn get_best_action_expectimax(&self, brain: &NTupleNetwork) -> u32 {
        let mut best_val = -f64::MAX;
        let mut best_action = 0;

        for action_idx in 0..4 {
            let dir = match action_idx {
                0 => Direction::Up,
                1 => Direction::Down,
                2 => Direction::Left,
                3 => Direction::Right,
                _ => continue,
            };

            let outcomes = self.game.get_all_possible_outcomes(dir);

            if !outcomes.is_empty() {
                let mut expected_value = 0.0;
                for outcome in &outcomes {
                    let future_value = brain.predict_game(outcome);
                    let move_quality = future_value;

                    expected_value += move_quality;
                }
                expected_value /= outcomes.len() as f64;

                if expected_value > best_val {
                    best_val = expected_value;
                    best_action = action_idx;
                }
            }
        }
        best_action
    }

    // pub fn get_best_action_expectimax_depth2(&self, brain: &NTupleNetwork) -> u32 {
    //     let mut best_val = f64::NEG_INFINITY;
    //     let mut best_action = 0;

    //     // --- PLY 1: MAX NODE (AI ch·ªçn h∆∞·ªõng ƒëi hi·ªán t·∫°i) ---
    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // L·∫•y t·∫•t c·∫£ c√°c k·ªãch b·∫£n g·∫°ch m·ªçc c√≥ th·ªÉ x·∫£y ra t·ª´ h∆∞·ªõng ƒëi n√†y
    //         // H√†m n√†y c·ªßa Huy ƒë√£ x·ª≠ l√Ω: Tr∆∞·ª£t -> L·∫•y Hint -> Nh√¢n ch√©o V·ªã tr√≠ x Gi√° tr·ªã
    //         let outcomes = self.game.get_all_possible_outcomes(dir);

    //         if !outcomes.is_empty() {
    //             let mut expected_value_ply1 = 0.0;

    //             // Duy·ªát qua t·ª´ng k·ªãch b·∫£n g·∫°ch m·ªçc (X√°c su·∫•t chia ƒë·ªÅu nh∆∞ Huy n√≥i)
    //             for outcome_game in &outcomes {
    //                 // --- PLY 2: MAX NODE (AI t√¨m ph·∫£n x·∫° t·ªët nh·∫•t t·ª´ k·ªãch b·∫£n ƒë√≥ - DEPTH 2) ---
    //                 // Thay v√¨ ch·∫•m ƒëi·ªÉm tƒ©nh, ta d√πng logic Afterstate ƒë·ªÉ t√¨m n∆∞·ªõc ƒëi "tho√°t hi·ªÉm" nh·∫•t
    //                 let best_future_score =
    //                     self.get_max_afterstate_score_from_game(outcome_game, brain);

    //                 expected_value_ply1 += best_future_score;
    //             }

    //             // T√≠nh trung b√¨nh c·ªông x√°c su·∫•t (Huy: 100% cho 1,2,3 ho·∫∑c 33% cho Bonus)
    //             expected_value_ply1 /= outcomes.len() as f64;

    //             if expected_value_ply1 > best_val {
    //                 best_val = expected_value_ply1;
    //                 best_action = action;
    //             }
    //         }
    //     }
    //     best_action as u32
    // }

    // pub fn get_best_action_parallel_depth5(&self, brain: &NTupleNetwork) -> u32 {
    //     let actions = vec![0, 1, 2, 3];

    //     // Song song h√≥a 4 h∆∞·ªõng ƒëi ch√≠nh
    //     let action_results: Vec<(u32, f64)> = actions
    //         .par_iter()
    //         .map(|&action| {
    //             let dir = match action {
    //                 0 => Direction::Up,
    //                 1 => Direction::Down,
    //                 2 => Direction::Left,
    //                 3 => Direction::Right,
    //                 _ => return (action, f64::NEG_INFINITY),
    //             };

    //             let outcomes = self.game.get_all_possible_outcomes(dir);
    //             if outcomes.is_empty() {
    //                 return (action, f64::NEG_INFINITY);
    //             }

    //             // ·ªû m·ªói nh√°nh, ta g·ªçi h√†m ƒë·ªá quy expectimax_node
    //             let total_val: f64 = outcomes
    //                 .iter()
    //                 .map(|outcome_game| {
    //                     self.expectimax_node(outcome_game, 4, brain) // depth 4 v√¨ ply 1 ƒë√£ xong
    //                 })
    //                 .sum();

    //             (action, total_val / outcomes.len() as f64)
    //         })
    //         .collect();

    //     // L·∫•y action c√≥ ƒëi·ªÉm k·ª≥ v·ªçng cao nh·∫•t
    //     action_results
    //         .into_iter()
    //         .max_by(|a, b| a.1.partial_cmp(&b.1).unwrap())
    //         .map(|(act, _)| act)
    //         .unwrap_or(0)
    // }

    // pub fn get_best_action_recursive(&self, brain: &NTupleNetwork, depth: u32) -> u32 {
    //     let mut best_val = -f64::MAX;
    //     let mut best_action = 0;

    //     // PLY 1: AI ch·ªçn n∆∞·ªõc ƒëi ƒë·∫ßu ti√™n
    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // L·∫•y danh s√°ch c√°c k·ªãch b·∫£n g·∫°ch m·ªçc (Chance Node 1)
    //         // (H√†m n√†y c·ªßa Huy ƒë√£ t·ª± update tracker v√† hint r·ªìi)
    //         let outcomes = self.game.get_all_possible_outcomes(dir);

    //         if !outcomes.is_empty() {
    //             let mut expected_value = 0.0;

    //             // T√≠nh gi√° tr·ªã trung b√¨nh c·ªßa c√°c k·ªãch b·∫£n
    //             for outcome_game in &outcomes {
    //                 // G·ªçi ƒë·ªá quy xu·ªëng c√°c t·∫ßng s√¢u h∆°n
    //                 // depth = SEARCH_DEPTH - 1 v√¨ ta ƒë√£ ƒëi xong n∆∞·ªõc ƒë·∫ßu ti√™n
    //                 expected_value += self.expectimax_node(outcome_game, depth - 1, brain);
    //             }

    //             expected_value /= outcomes.len() as f64;

    //             if expected_value > best_val {
    //                 best_val = expected_value;
    //                 best_action = action;
    //             }
    //         }
    //     }

    //     // Fallback: N·∫øu kh√¥ng ƒë∆∞·ªùng n√†o ƒëi ƒë∆∞·ª£c (best_action v·∫´n = 0 v√† val √¢m v√¥ c√πng)
    //     // th√¨ c·ª© tr·∫£ v·ªÅ 0, game s·∫Ω t·ª± x·ª≠ l√Ω Game Over sau.
    //     best_action as u32
    // }

    // /// H√†m ƒë·ªá quy x·ª≠ l√Ω c√°c t·∫ßng ti·∫øp theo
    // /// Input: game state ƒë√£ m·ªçc g·∫°ch xong (ƒëang ch·ªù AI ƒëi n∆∞·ªõc ti·∫øp theo)
    // fn expectimax_node(&self, game: &Game, depth: u32, brain: &NTupleNetwork) -> f64 {
    //     // BASE CASE: N·∫øu ƒë√£ ch·∫°m ƒë√°y ƒë·ªô s√¢u
    //     // Ta d√πng Afterstate (nh√¨n th√™m 1 b∆∞·ªõc ƒë·ªám) ƒë·ªÉ ƒë√°nh gi√° l√° ch·∫Øn cu·ªëi c√πng
    //     if depth == 0 {
    //         return self.get_max_afterstate_score_from_game(game, brain);
    //     }

    //     let mut max_val = -f64::MAX;
    //     let mut can_move = false;

    //     // MAX NODE: AI ch·ªçn n∆∞·ªõc ƒëi t·ªët nh·∫•t t·∫°i t·∫ßng n√†y
    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // CHANCE NODE: Sinh ra c√°c k·ªãch b·∫£n g·∫°ch m·ªçc ti·∫øp theo
    //         let outcomes = game.get_all_possible_outcomes(dir);

    //         if !outcomes.is_empty() {
    //             can_move = true;
    //             let mut current_dir_expected_val = 0.0;

    //             for outcome_game in &outcomes {
    //                 // ƒê·ªÜ QUY TI·∫æP: Xu·ªëng t·∫ßng s√¢u h∆°n (depth - 1)
    //                 current_dir_expected_val +=
    //                     self.expectimax_node(outcome_game, depth - 1, brain);
    //             }

    //             // T√≠nh trung b√¨nh (Average)
    //             current_dir_expected_val /= outcomes.len() as f64;

    //             // Maximize: AI s·∫Ω ch·ªçn h∆∞·ªõng c√≥ k·ª≥ v·ªçng cao nh·∫•t
    //             if current_dir_expected_val > max_val {
    //                 max_val = current_dir_expected_val;
    //             }
    //         }
    //     }

    //     // N·∫øu t·∫°i t·∫ßng n√†y kh√¥ng ƒëi ƒë∆∞·ª£c ƒë√¢u n·ªØa -> Game Over -> ƒêi·ªÉm 0
    //     if !can_move {
    //         return 0.0;
    //     }

    //     max_val
    // }

    // fn get_max_afterstate_score_from_game(&self, game: &Game, brain: &NTupleNetwork) -> f64 {
    //     let mut max_val = f64::NEG_INFINITY;
    //     let mut can_move_any = false;

    //     // Th·ª≠ 4 h∆∞·ªõng ph·∫£n x·∫° ti·∫øp theo
    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // S·ª¨ D·ª§NG TR·ª∞C TI·∫æP H√ÄM C·ª¶A HUY T·∫†I ƒê√ÇY
    //         if let Some(after_board) = game.get_afterstate(dir) {
    //             can_move_any = true;

    //             // Chuy·ªÉn board sang d·∫°ng ph·∫≥ng ƒë·ªÉ N-Tuple predict
    //             let mut flat = [0u32; 16];
    //             for r in 0..4 {
    //                 for c in 0..4 {
    //                     flat[r * 4 + c] = after_board[r][c].value;
    //                 }
    //             }

    //             let score = brain.predict(&flat);
    //             if score > max_val {
    //                 max_val = score;
    //             }
    //         }
    //     }

    //     // N·∫øu b∆∞·ªõc ti·∫øp theo kh√¥ng ƒëi ƒë∆∞·ª£c h∆∞·ªõng n√†o (Game Over gi·∫£ l·∫≠p)
    //     if !can_move_any {
    //         return 0.0; // Ph·∫°t n·∫∑ng v√¨ n∆∞·ªõc ƒëi tr∆∞·ªõc ƒë√≥ d·∫´n ƒë·∫øn ng√µ c·ª•t
    //     }

    //     max_val
    // }

    /// H√†m Wrapper: G·ªçi t·ª´ b√™n ngo√†i v·ªõi s·ªë Ply mong mu·ªën
    /// V√≠ d·ª•: ply = 1 (ch·ªâ tr∆∞·ª£t), ply = 4 (tr∆∞·ª£t-m·ªçc-tr∆∞·ª£t-m·ªçc)
    pub fn get_best_action_ply(&self, brain: &NTupleNetwork, depth: u32) -> (u32, f64) {
        let mut best_val = f64::NEG_INFINITY;
        let mut best_action = 0;
        let mut found_valid_move = false;

        // Duy·ªát 4 h∆∞·ªõng (Up, Down, Left, Right)
        for action in 0..4 {
            let dir = match action {
                0 => Direction::Up,
                1 => Direction::Down,
                2 => Direction::Left,
                3 => Direction::Right,
                _ => continue,
            };

            // PRUNING: N·∫øu kh√¥ng ƒëi ƒë∆∞·ª£c, b·ªè qua ngay
            if !self.game.can_move(dir) {
                continue;
            }

            // B∆Ø·ªöC QUAN TR·ªåNG:
            let val = self.search_chance_node(&self.game, dir, depth, brain);

            // --- S·ª¨A ·ªû ƒê√ÇY ---
            // Logic c≈©: if val > best_val { ... } -> SAI n·∫øu val = -inf
            // Logic m·ªõi: N·∫øu ƒë√¢y l√† n∆∞·ªõc ƒëi h·ª£p l·ªá ƒë·∫ßu ti√™n t√¨m th·∫•y -> L·∫§Y LU√îN
            // Ho·∫∑c n·∫øu ƒëi·ªÉm cao h∆°n ƒëi·ªÉm c≈© -> L·∫§Y
            if !found_valid_move || val > best_val {
                best_val = val;
                best_action = action;
                found_valid_move = true; // ƒê√°nh d·∫•u l√† ƒë√£ c√≥ √≠t nh·∫•t 1 n∆∞·ªõc ƒëi
            }
        }

        // X·ª≠ l√Ω Game Over
        if !found_valid_move {
            // Tr·∫£ v·ªÅ 0 v√† ƒëi·ªÉm ph·∫°t.
            // L∆ØU √ù: B√™n ngo√†i g·ªçi h√†m n√†y ph·∫£i check game over tr∆∞·ªõc ho·∫∑c x·ª≠ l√Ω ƒëi·ªÉm ph·∫°t n√†y
            return (100, brain.predict_game(&self.game));
        }

        (best_action as u32, best_val)
    }

    fn search_chance_node(
        &self,
        game: &Game,
        dir: Direction,
        depth: u32,
        brain: &NTupleNetwork,
    ) -> f64 {
        // --- ƒêI·ªÇM D·ª™NG (LEAF NODE) ---
        // N·∫øu depth = 1, ta ch·ªâ ƒë√°nh gi√° Afterstate (B√†n c·ªù v·ª´a tr∆∞·ª£t xong, ch∆∞a m·ªçc qu√¢n)
        // ƒê√¢y l√† k·ªπ thu·∫≠t t·ªëi ∆∞u t·ªëc ƒë·ªô (nh√¨n ng·∫Øn 1 b∆∞·ªõc).
        if depth == 1 {
            // V√¨ root ƒë√£ check can_move, h√†m n√†y CH·∫ÆC CH·∫ÆN tr·∫£ v·ªÅ Some.
            // D√πng expect ƒë·ªÉ kh·∫≥ng ƒë·ªãnh t√≠nh ƒë√∫ng ƒë·∫Øn c·ªßa logic.
            let after_board = game
                .get_afterstate(dir)
                .expect("üî• BUG Logic: Root check can_move OK nh∆∞ng get_afterstate tr·∫£ None");

            let mut flat = [0u32; 16];
            for i in 0..16 {
                flat[i] = after_board[i / 4][i % 4].value;
            }
            return brain.predict(&flat);
        }

        // --- SINH QU√ÇN (SPAWN) ---
        // Sinh ra c√°c outcomes (B√†n c·ªù ƒë√£ m·ªçc qu√¢n m·ªõi)
        let outcomes = game.get_all_possible_outcomes_pure(dir);

        if outcomes.is_empty() {
            // N·∫øu kh√¥ng sinh ƒë∆∞·ª£c outcome n√†o d√π can_move = true => L·ªói logic game nghi√™m tr·ªçng
            unreachable!("üî• BUG Logic: Move valid nh∆∞ng kh√¥ng sinh ƒë∆∞·ª£c outcome n√†o!");
        }

        // --- T√çNH TRUNG B√åNH (AVERAGE) ---
        let mut total_score = 0.0;
        let prob = 1.0 / outcomes.len() as f64; // Gi·∫£ s·ª≠ x√°c su·∫•t ƒë·ªÅu

        for outcome_game in &outcomes {
            if depth == 2 {
                // N·∫øu c√≤n 2 ply (Move + Spawn), ƒë√¢y l√† t·∫ßng cu·ªëi c√πng.
                // ƒê√°nh gi√° tr·ª±c ti·∫øp Game State sau khi m·ªçc qu√¢n.
                total_score += prob * brain.predict_game(outcome_game);
            } else {
                // N·∫øu depth > 2, ti·∫øp t·ª•c g·ªçi ƒë·ªá quy sang l∆∞·ª£t ng∆∞·ªùi ch∆°i (Max Node)
                // Gi·∫£m depth ƒëi 2 (t∆∞∆°ng ·ª©ng v·ªõi 1 c·∫∑p Move + Spawn ƒë√£ ho√†n th√†nh)
                let val = self.search_max_node(outcome_game, depth - 2, brain);
                total_score += prob * val;
            }
        }

        total_score
    }

    fn search_max_node(&self, game: &Game, depth: u32, brain: &NTupleNetwork) -> f64 {
        // ƒêi·ªÅu ki·ªán d·ª´ng ƒë·ªá quy (th∆∞·ªùng kh√¥ng ch·∫°y v√†o ƒë√¢y n·∫øu logic tr√™n chu·∫©n, nh∆∞ng c·ª© ƒë·ªÉ an to√†n)
        if depth == 0 {
            return brain.predict_game(game);
        }

        let mut best_val = f64::NEG_INFINITY;
        let mut can_move_any = false;

        // Duy·ªát 4 h∆∞·ªõng
        for action in 0..4 {
            let dir = match action {
                0 => Direction::Up,
                1 => Direction::Down,
                2 => Direction::Left,
                3 => Direction::Right,
                _ => continue,
            };

            // PRUNING: C·∫Øt nh√°nh c·ª•t
            if game.can_move(dir) {
                can_move_any = true;
                // G·ªçi quay l·∫°i Chance Node ƒë·ªÉ x·ª≠ l√Ω ti·∫øp
                let val = self.search_chance_node(game, dir, depth, brain);

                if val > best_val {
                    best_val = val;
                }
            }
        }

        // X·ª¨ L√ù DEAD END (GAME OVER)
        if !can_move_any {
            // N·∫øu kh√¥ng ƒëi ƒë∆∞·ª£c h∆∞·ªõng n√†o, tr·∫£ v·ªÅ ƒëi·ªÉm ph·∫°t.
            // ƒêi·ªÉm ph·∫°t ch√≠nh l√† ƒëi·ªÉm ƒë√°nh gi√° c·ªßa b√†n c·ªù ch·∫øt n√†y (th∆∞·ªùng th·∫•p).
            return brain.predict_game(game);
        }

        best_val
    }

    // pub fn get_best_action_afterstate(&self, brain: &NTupleNetwork) -> u32 {
    //     let mut best_action = 0;
    //     let mut best_val = f64::NEG_INFINITY;

    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         if let Some(after_board) = self.game.get_afterstate(dir) {
    //             let mut flat = [0u32; 16];
    //             for r in 0..4 {
    //                 for c in 0..4 {
    //                     flat[r * 4 + c] = after_board[r][c].value;
    //                 }
    //             }

    //             let val = brain.predict(&flat);
    //             if val > best_val {
    //                 best_val = val;
    //                 best_action = action;
    //             }
    //         }
    //     }
    //     best_action as u32
    // }

    // /// Afterstate Recursive d√πng ƒë·ªÉ Gen Data
    // pub fn get_best_action_afterstate_recursive(&self, brain: &NTupleNetwork, depth: u32) -> u32 {
    //     let mut best_val = f64::NEG_INFINITY;
    //     let mut best_action = 0;

    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // Ply 1: AI th·ª±c hi·ªán tr∆∞·ª£t board
    //         if let Some(after_board) = self.game.get_afterstate(dir) {
    //             let mut temp_game = self.game.clone();
    //             temp_game.board = after_board;

    //             // ƒêi s√¢u xu·ªëng c√°c t·∫ßng ti·∫øp theo (m·ªói t·∫ßng l√† 1 c·∫∑p: Spawn + Move)
    //             let val = self.afterstate_node_recursive(&temp_game, depth - 1, brain);

    //             if val > best_val {
    //                 best_val = val;
    //                 best_action = action;
    //             }
    //         }
    //     }
    //     best_action as u32
    // }

    // fn afterstate_node_recursive(&self, game: &Game, depth: u32, brain: &NTupleNetwork) -> f64 {
    //     if depth <= 1 {
    //         return brain.predict_game(game);
    //     }

    //     let mut total_expected_val = 0.0;
    //     let mut move_count = 0;

    //     // Duy·ªát qua 4 h∆∞·ªõng ƒë·ªÉ t√¨m c√°c k·ªãch b·∫£n m·ªçc g·∫°ch ti·∫øp theo
    //     for action in 0..4 {
    //         let dir = match action {
    //             0 => Direction::Up,
    //             1 => Direction::Down,
    //             2 => Direction::Left,
    //             3 => Direction::Right,
    //             _ => continue,
    //         };

    //         // TRUY·ªÄN DIR V√ÄO ƒê√ÇY
    //         let outcomes = game.get_all_possible_outcomes_pure(dir);

    //         if outcomes.is_empty() {
    //             continue;
    //         }

    //         move_count += 1;
    //         let mut best_next_val = f64::NEG_INFINITY;

    //         for outcome_game in &outcomes {
    //             // ƒê·ªá quy xu·ªëng t·∫ßng s√¢u h∆°n
    //             let val = self.afterstate_node_recursive(outcome_game, depth - 1, brain);
    //             if val > best_next_val {
    //                 best_next_val = val;
    //             }
    //         }

    //         total_expected_val += if best_next_val == f64::NEG_INFINITY {
    //             0.0
    //         } else {
    //             best_next_val
    //         };
    //     }

    //     if move_count == 0 {
    //         return 0.0;
    //     }
    //     total_expected_val / move_count as f64
    // }

    pub fn train_step(&mut self, brain: &mut NTupleNetwork, action: u32, alpha: f64) -> (f64, f64) {
        // 1. L·∫•y Afterstate hi·ªán t·∫°i (S_after)
        let dir = Direction::from_u32(action);

        let after_board = match self.game.get_afterstate(dir) {
            Some(board) => board,
            None => return (0.0, -10.0), // Tr·∫£ v·ªÅ l·ªói n·∫øu action kh√¥ng h·ª£p l·ªá
        };

        // Flatten board ƒë·ªÉ ƒë∆∞a v√†o m·∫°ng
        let mut s_after_flat = [0u32; 16];
        for r in 0..4 {
            for c in 0..4 {
                s_after_flat[r * 4 + c] = after_board[r][c].value;
            }
        }

        // 2. D·ª± ƒëo√°n V(S_after) t·ª´ m·∫°ng
        let v_after = brain.predict(&s_after_flat);

        // 3. Th·ª±c hi·ªán h√†nh ƒë·ªông th·∫≠t (Environment Step)
        let score_old = self.game.score;
        let (_, _, done, _) = self.step(action);
        let reward = (self.game.score - score_old) as f64;

        // 4. T√≠nh Target V(S'_after) cho b∆∞·ªõc ti·∫øp theo
        // QUAN TR·ªåNG: Lu√¥n d√πng Ply = 1 (1-step lookahead) ƒë·ªÉ t√≠nh Target chu·∫©n x√°c
        let v_next_after = if done {
            0.0
        } else {
            // G·ªçi h√†m t√¨m best action nh∆∞ng FIX C·ª®NG DEPTH = 1
            let (best_action, best_val) = self.get_best_action_ply(brain, 1);

            // Ki·ªÉm tra sentinel value 100 (t·∫Øc ƒë∆∞·ªùng) ho·∫∑c gi√° tr·ªã v√¥ c·ª±c
            if best_action == 100 || best_val == f64::NEG_INFINITY {
                0.0
            } else {
                best_val
            }
        };

        // 5. T√≠nh TD Error
        let td_error = reward + self.gamma * v_next_after - v_after;

        // 6. C·∫≠p nh·∫≠t Traces & Weights
        let effective_alpha = alpha / brain.tuples.len() as f64;

        // Lazy init traces
        if self.traces.is_empty() {
            let sizes: Vec<usize> = brain.weights.iter().map(|w| w.len()).collect();
            self.reset_traces(brain.weights.len(), &sizes);
        }

        brain.update_weights_td_lambda(
            &mut self.traces,
            &mut self.active_trace_indices,
            &s_after_flat,
            td_error,
            effective_alpha,
        );

        (td_error.abs(), reward)
    }

    pub fn reset(&mut self) -> (Vec<u32>, Vec<u32>) {
        self.game = Game::new();

        // Reset traces khi game m·ªõi b·∫Øt ƒë·∫ßu (n·∫øu ƒë√£ ƒë∆∞·ª£c init)
        if !self.traces.is_empty() {
            // L∆∞u √Ω: reset_traces ·ªü ƒë√¢y ch·ªâ clear gi√° tr·ªã, kh√¥ng re-alloc
            // Ta d√πng logic clear nhanh ƒë√£ vi·∫øt trong reset_traces
            for (table_idx, indices) in self.active_trace_indices.iter_mut().enumerate() {
                for &feat_idx in indices.iter() {
                    self.traces[table_idx][feat_idx] = 0.0;
                }
                indices.clear();
            }
        }

        (self.get_board_flat().to_vec(), self.game.hints.clone())
    }

    pub fn get_board_flat(&self) -> [u32; 16] {
        // ·ª¶y quy·ªÅn tr·ª±c ti·∫øp cho Game x·ª≠ l√Ω
        self.game.get_board_flat()
    }
}
