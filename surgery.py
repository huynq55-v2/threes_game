import os
import torch
import torch.nn as nn
import torch.nn.functional as F
import gymnasium as gym
import numpy as np
from gymnasium import spaces
from stable_baselines3.common.vec_env import SubprocVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback
from stable_baselines3.common.torch_layers import BaseFeaturesExtractor
from sb3_contrib import MaskablePPO
from sb3_contrib.common.wrappers import ActionMasker
from stable_baselines3.common.monitor import Monitor
import threes_rs  # Th∆∞ vi·ªán Rust c·ªßa b·∫°n

# ==========================================
# PH·∫¶N 1: M√îI TR∆Ø·ªúNG (WRAPPER)
# ==========================================
class ThreesGymEnv(gym.Env):
    metadata = {"render_modes": ["human"]}

    def __init__(self):
        super().__init__()
        # Kh·ªüi t·∫°o game Rust
        self.game = threes_rs.ThreesEnv() 
        
        self.action_space = spaces.Discrete(4)
        
        self.observation_space = spaces.Dict({
            "board": spaces.Box(low=0, high=15, shape=(1, 4, 4), dtype=np.float32),
            "hint": spaces.Box(low=0, high=1, shape=(13,), dtype=np.float32),
        })
        
        self.TILE_MAP = {v: i for i, v in enumerate([1, 2, 3, 6, 12, 24, 48, 96, 192, 384, 768, 1536, 3072, 6144])}

        self.current_episode_reward = 0.0

    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        raw_board, raw_hint_set = self.game.reset()

        self.current_episode_reward = 0.0

        observation = self._process_obs(raw_board, raw_hint_set)
        return observation, {}

    def step(self, action):
        next_board, reward, done, next_hint_set = self.game.step(int(action))
        
        # --- TH√äM ƒêO·∫†N N√ÄY ƒê·ªÇ IN LOG RA M√ÄN H√åNH ---
        # TH√äM D√íNG N√ÄY: C·ªông d·ªìn reward v√†o t·ªïng
        self.current_episode_reward += reward
        
        # S·ª≠a ƒëo·∫°n print
        if done:
            max_val = max(next_board)
            
            # In ra T·ªîNG REWARD (self.current_episode_reward) thay v√¨ reward b∆∞·ªõc cu·ªëi
            print(f"üíÄ Die! MaxTile: {int(max_val)} | Total Reward: {self.current_episode_reward:.2f}")
        # -------------------------------------------

        # Scale reward
        reward = reward * 0.1 
        
        observation = self._process_obs(next_board, next_hint_set)
        return observation, reward, done, False, {}

    def valid_action_mask(self):
        valid_moves = self.game.valid_moves() 
        return np.array(valid_moves, dtype=bool)

    def _process_obs(self, flat_board, hint_set):
        # 1. Board
        board_np = np.array(flat_board, dtype=np.float32)
        ranks = np.zeros_like(board_np)
        ranks[board_np == 1] = 1
        ranks[board_np == 2] = 2
        mask = (board_np >= 3)
        ranks[mask] = np.floor(np.log2(board_np[mask] / 3.0) + 1e-5) + 3
        ranks = np.clip(ranks, 0, 15)
        board_final = ranks.reshape(1, 4, 4)
        
        # 2. Hint
        hint_vec = np.zeros((13,), dtype=np.float32)
        for h in hint_set:
            if h in self.TILE_MAP:
                hint_vec[self.TILE_MAP[h]] = 1.0
                
        return {"board": board_final, "hint": hint_vec}

    def undo(self):
        # G·ªçi h√†m undo t·ª´ Rust
        board_flat, reward, done, hint_set = self.game.undo()
        # Chuy·ªÉn ƒë·ªïi sang observation format m√† Model hi·ªÉu
        obs = self._process_obs(board_flat, hint_set)
        return obs

    def redo(self):
        board_flat, reward, done, hint_set = self.game.redo()
        obs = self._process_obs(board_flat, hint_set)
        return obs

# --- H√ÄM MAKE ENV (QUAN TR·ªåNG: Ph·∫£i ƒë·ªãnh nghƒ©a ·ªü ƒë√¢y ƒë·ªÉ multiprocessing g·ªçi ƒë∆∞·ª£c) ---
def make_env():
    env = ThreesGymEnv()
    # 1. Action Masker
    env = ActionMasker(env, lambda env: env.valid_action_mask())
    # 2. Monitor: QUAN TR·ªåNG NH·∫§T ƒê·ªÇ HI·ªÜN LOG SB3
    # N√≥ s·∫Ω ghi l·∫°i Reward v√† Moves ƒë·ªÉ hi·ªÉn th·ªã trong b·∫£ng log
    env = Monitor(env)
    return env

# ==========================================
# PH·∫¶N 2: M·∫†NG RESNET (FEATURE EXTRACTOR)
# ==========================================
class ResidualBlock(nn.Module):
    def __init__(self, channels):
        super().__init__()
        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn1 = nn.BatchNorm2d(channels)
        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1, bias=False)
        self.bn2 = nn.BatchNorm2d(channels)

    def forward(self, x):
        residual = x
        out = F.gelu(self.bn1(self.conv1(x)))
        out = self.bn2(self.conv2(out))
        out += residual
        out = F.gelu(out)
        return out

class ThreesResNetExtractor(BaseFeaturesExtractor):
    def __init__(self, observation_space: gym.spaces.Dict, features_dim: int = 512):
        super().__init__(observation_space, features_dim)
        
        # A. Board Branch
        self.embedding = nn.Embedding(16, 64)
        self.resnet = nn.Sequential(
            nn.Conv2d(64, 128, kernel_size=3, padding=1, bias=False),
            nn.BatchNorm2d(128),
            nn.GELU(),
            ResidualBlock(128),
            ResidualBlock(128),
        )
        self.board_out_dim = 128 * 4 * 4
        
        # B. Hint Branch
        self.hint_net = nn.Sequential(
            nn.Linear(14, 64),
            nn.GELU(),
            nn.Linear(64, 64),
            nn.GELU()
        )
        self.hint_out_dim = 64
        
        # C. Fusion
        combined_dim = self.board_out_dim + self.hint_out_dim
        self.fusion = nn.Sequential(
            nn.Linear(combined_dim, features_dim),
            nn.GELU()
        )

    def forward(self, observations):
        board = observations["board"].long().squeeze(1).clamp(0, 15)
        hint = observations["hint"]
        
        # Board Path
        x = self.embedding(board)           
        x = x.permute(0, 3, 1, 2)           
        board_feat = self.resnet(x)         
        board_feat = board_feat.flatten(1)  
        
        # Hint Path
        hint_feat = self.hint_net(hint)     
        
        # Combine
        combined = torch.cat((board_feat, hint_feat), dim=1)
        return self.fusion(combined)

def surgery_on_checkpoint(old_path, new_path, env):
    # --- ƒê·ªäNH NGHƒ®A L·∫†I POLICY_KWARGS CHO CH·∫ÆC CH·∫ÆN ---
    # (Ph·∫£i tr√πng kh·ªõp v·ªõi ki·∫øn tr√∫c b√°c ƒë√£ d√πng ƒë·ªÉ train 9.3M steps)
    policy_kwargs = dict(
        features_extractor_class=ThreesResNetExtractor,
        features_extractor_kwargs=dict(features_dim=512),
        net_arch=dict(pi=[256, 256], vf=[256, 256]),
        activation_fn=nn.GELU,
    )

    print("üè• B·∫Øt ƒë·∫ßu ca ph·∫´u thu·∫≠t...")
    
    # 1. Kh·ªüi t·∫°o model m·ªõi v·ªõi ki·∫øn tr√∫c 14 input (ƒë√£ s·ª≠a trong Env c·ªßa b√°c)
    new_model = MaskablePPO(
        "MultiInputPolicy",
        env,
        policy_kwargs=policy_kwargs, # Ph·∫£i ƒë·∫£m b·∫£o policy_kwargs ƒë√£ update features_dim
        verbose=1
    )

    # 2. Load tr·ªçng s·ªë t·ª´ model c≈©
    # L∆∞u √Ω: load v·ªõi device="cpu" cho an to√†n
    old_model = MaskablePPO.load(old_path, device="cpu")
    old_params = old_model.policy.state_dict()
    new_params = new_model.policy.state_dict()

    print("üß† ƒêang chuy·ªÉn giao k√Ω ·ª©c...")
    for key in new_params.keys():
        if key in old_params:
            if new_params[key].shape == old_params[key].shape:
                # N·∫øu shape kh·ªõp (ph·∫ßn ResNet, ph·∫ßn Fusion), ch√©p nguy√™n sang
                new_params[key].copy_(old_params[key])
            else:
                # N·∫øu l·ªách shape (ch√≠nh l√† l·ªõp Linear ƒë·∫ßu ti√™n c·ªßa Hint)
                print(f"‚úÇÔ∏è  ƒêang kh√¢u v·∫øt m·ªï t·∫°i: {key}")
                old_weight = old_params[key] # Shape [64, 13]
                # Ch√©p 13 c·ªôt c≈© v√†o 13 c·ªôt ƒë·∫ßu c·ªßa model m·ªõi [64, 14]
                new_params[key][:, :13].copy_(old_weight)
                # C·ªôt th·ª© 14 ƒë·ªÉ m·∫∑c ƒë·ªãnh (init l√† 0 ho·∫∑c random nh·ªè)
        else:
            print(f"‚ö†Ô∏è  Ph√°t hi·ªán v√πng n√£o m·ªõi: {key}")

    # 3. C·∫≠p nh·∫≠t tr·ªçng s·ªë m·ªõi v√†o model m·ªõi
    new_model.policy.load_state_dict(new_params)
    
    # 4. L∆∞u l·∫°i b·∫£n "h·ªìi sinh"
    new_model.save(new_path)
    print(f"‚úÖ Ph·∫´u thu·∫≠t th√†nh c√¥ng! File m·ªõi ƒë√£ s·∫µn s√†ng t·∫°i: {new_path}")

# --- TH·ª∞C THI ---
if __name__ == "__main__":
    # Nh·ªõ kh·ªüi t·∫°o env m·ªõi v·ªõi shape 14 tr∆∞·ªõc khi g·ªçi h√†m n√†y
    test_env = make_env() 
    old_ckpt = "./logs_ppo_threes_resnet/ppo_resnet_9600000_steps.zip"
    new_ckpt = "./logs_ppo_threes_resnet/ppo_resnet_9600000_v2_14input.zip"
    
    surgery_on_checkpoint(old_ckpt, new_ckpt, test_env)