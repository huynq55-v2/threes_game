{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":735728,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":560858,"modelId":573478}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\n\nBASE_DIR = \"/kaggle/input/threes-binary\"\n\ndef find_latest_checkpoint(base_dir, filename=\"threes_pro_latest.pth\"):\n    versions = []\n\n    # duy·ªát to√†n b·ªô c√¢y th∆∞ m·ª•c\n    for root, dirs, files in os.walk(base_dir):\n        if filename in files:\n            # l·∫•y version t·ª´ path: .../default/<version>/\n            parts = root.split(os.sep)\n            for p in reversed(parts):\n                if p.isdigit():\n                    versions.append((int(p), os.path.join(root, filename)))\n                    break\n\n    if not versions:\n        raise FileNotFoundError(f\"Kh√¥ng t√¨m th·∫•y {filename}\")\n\n    # l·∫•y version l·ªõn nh·∫•t\n    versions.sort(key=lambda x: x[0], reverse=True)\n    return versions[0][1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RUST_BINARY = find_latest_checkpoint(BASE_DIR, \"threes_game-0.1.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\")\n!pip install --force-reinstall \"{RUST_BINARY}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T02:09:58.760095Z","iopub.execute_input":"2026-01-30T02:09:58.760425Z","iopub.status.idle":"2026-01-30T02:10:02.978584Z","shell.execute_reply.started":"2026-01-30T02:09:58.760396Z","shell.execute_reply":"2026-01-30T02:10:02.977850Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import threes_rs\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport numpy as np\nimport random\nimport math\nimport os\nimport time\nfrom collections import deque\nimport threes_rs  # Rust binding\n\n# --- C·∫§U H√åNH (CONFIGURATION) ---\nNUM_ENVS = 128          # S·ªë l∆∞·ª£ng m√¥i tr∆∞·ªùng ch·∫°y song song\nBATCH_SIZE = 256        # K√≠ch th∆∞·ªõc batch training\nGAMMA = 0.99            # Discount factor\nLR = 1e-4               # Learning rate\nTARGET_UPDATE = 5000    # T·∫ßn su·∫•t c·∫≠p nh·∫≠t m·∫°ng target\nMEMORY_SIZE = 500000    # Replay Buffer size\nEPS_DECAY = 500000      # T·ªëc ƒë·ªô gi·∫£m Epsilon (c√†ng l·ªõn c√†ng ch·∫≠m -> kh√°m ph√° l√¢u h∆°n)\n\nCHECKPOINT_FILE = find_latest_checkpoint(BASE_DIR)\nCHECKPOINT_SAVE = \"/kaggle/working/threes_pro_latest.pth\"\n\nprint(\"Using checkpoint:\", CHECKPOINT_FILE)\n\n# Mapping 13 lo·∫°i qu√¢n (1, ... 3072) -> Index (0..12)\nTILE_TYPES = [1, 2, 3, 6, 12, 24, 48, 96, 192, 384, 768, 1536, 3072]\nTILE_MAP = {v: i for i, v in enumerate(TILE_TYPES)}\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# --- HELPER CLASSES ---\n\nclass DataAugmenter:\n    \"\"\"Handles 8x Data Augmentation (Rotations & Flips) using Vectorized NumPy ops.\"\"\"\n    def __init__(self):\n        # Pre-computed permutations for 4x4 board (flattened to 16)\n        self.BOARD_PERMS = [\n            np.array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15]), # Id\n            np.array([12, 8, 4, 0, 13, 9, 5, 1, 14, 10, 6, 2, 15, 11, 7, 3]), # Rot90\n            np.array([15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]), # Rot180\n            np.array([3, 7, 11, 15, 2, 6, 10, 14, 1, 5, 9, 13, 0, 4, 8, 12]), # Rot270\n            np.array([3, 2, 1, 0, 7, 6, 5, 4, 11, 10, 9, 8, 15, 14, 13, 12]), # FlipX\n            np.array([0, 4, 8, 12, 1, 5, 9, 13, 2, 6, 10, 14, 3, 7, 11, 15]), # FlipMainDiag\n            np.array([12, 13, 14, 15, 8, 9, 10, 11, 4, 5, 6, 7, 0, 1, 2, 3]), # FlipY\n            np.array([15, 11, 7, 3, 14, 10, 6, 2, 13, 9, 5, 1, 12, 8, 4, 0]), # FlipAntiDiag\n        ]\n        \n        # Action Mappings: 0:Up, 1:Down, 2:Left, 3:Right\n        # Maps action index when board is transformed\n        self.ACTION_PERMS = [\n            np.array([0, 1, 2, 3]), # Id\n            np.array([3, 2, 0, 1]), # Rot90\n            np.array([1, 0, 3, 2]), # Rot180\n            np.array([2, 3, 1, 0]), # Rot270\n            np.array([0, 1, 3, 2]), # FlipX\n            np.array([3, 2, 0, 1])[np.array([0, 1, 3, 2])], # FlipMainDiag\n            np.array([1, 0, 3, 2])[np.array([0, 1, 3, 2])], # FlipY\n            np.array([2, 3, 1, 0])[np.array([0, 1, 3, 2])], # FlipAntiDiag\n        ]\n\n    def augment_batch(self, states, actions, rewards, next_states, dones):\n        \"\"\"\n        Input: Batch of N transitions.\n        Output: Batch of N * 8 transitions (Original + 7 Symmetries).\n        \"\"\"\n        # Split States into Board (N, 16) and Hints (N, 13)\n        hints = states[:, 16:]\n        next_hints = next_states[:, 16:]\n        boards = states[:, :16]\n        next_boards = next_states[:, :16]\n        \n        aug_states, aug_actions, aug_next_states = [], [], []\n        # Rewards and Dones are invariant to symmetry, so we just repeat them later\n        \n        actions_np = np.array(actions)\n\n        # Generate 8 variations\n        for i in range(8):\n            perm_b = self.BOARD_PERMS[i]\n            perm_a = self.ACTION_PERMS[i]\n            \n            # 1. Transform Boards\n            b_sym = boards[:, perm_b]\n            nb_sym = next_boards[:, perm_b]\n            \n            # 2. Transform Actions\n            a_sym = perm_a[actions_np]\n            \n            # 3. Reconstruct Full States (Board + Hint)\n            s_sym = np.concatenate([b_sym, hints], axis=1)\n            ns_sym = np.concatenate([nb_sym, next_hints], axis=1)\n            \n            aug_states.append(s_sym)\n            aug_actions.append(a_sym)\n            aug_next_states.append(ns_sym)\n\n        # Vectorized Concatenation\n        # Repeat rewards/dones 8 times\n        return (\n            np.concatenate(aug_states, axis=0),\n            np.concatenate(aug_actions, axis=0),\n            np.tile(rewards, 8),          # [R1..Rn, R1..Rn, ...]\n            np.concatenate(aug_next_states, axis=0),\n            np.tile(dones, 8)\n        )\n\n# --- M·∫†NG TH·∫¶N KINH (CNN ARCHITECTURE) ---\nclass DQN(nn.Module):\n    def __init__(self):\n        super(DQN, self).__init__()\n        self.embedding = nn.Embedding(16, 64) \n        \n        self.conv_net = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=2, stride=1),\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=2, stride=1),\n            nn.ReLU(),\n            nn.Flatten() \n        )\n\n        self.fc = nn.Sequential(\n            nn.Linear(1024 + 13, 512),\n            nn.ReLU(),\n            nn.Linear(512, 4)\n        )\n\n    def forward(self, state):\n        # state: (Batch, 29) -> First 16 are board indices, last 13 are hints\n        board = state[:, :16].long()\n        hints = state[:, 16:].float()\n        \n        # Reshape board to image: (Batch, 16) -> (Batch, 64, 4, 4)\n        x = self.embedding(board.clamp(0, 15))\n        x = x.view(-1, 4, 4, 64).permute(0, 3, 1, 2)\n        \n        conv_out = self.conv_net(x)\n        combined = torch.cat((conv_out, hints), dim=1)\n        return self.fc(combined)\n\ndef prepare_state_batch(boards_flat, hint_sets):\n    \"\"\"Converts raw outputs from Rust env to Neural Net input tensors.\"\"\"\n    n = len(boards_flat)\n    boards = np.array(boards_flat, dtype=np.float32)\n    \n    multi_hots = np.zeros((n, 13), dtype=np.float32)\n    for i, hints in enumerate(hint_sets):\n        for h in hints:\n            if h in TILE_MAP:\n                multi_hots[i, TILE_MAP[h]] = 1.0\n                \n    return np.concatenate([boards, multi_hots], axis=1)\n\n# --- REPLAY BUFFER ---\nclass ReplayBuffer:\n    def __init__(self, capacity):\n        self.buffer = deque(maxlen=capacity)\n    def push_batch(self, states, actions, rewards, next_states, dones):\n        # Efficient push loop\n        for i in range(len(states)):\n            self.buffer.append((states[i], actions[i], rewards[i], next_states[i], dones[i]))\n            \n    def sample(self, batch_size):\n        batch = random.sample(self.buffer, batch_size)\n        s, a, r, ns, d = zip(*batch)\n        return (torch.tensor(np.array(s), dtype=torch.float32, device=device),\n                torch.tensor(a, dtype=torch.long, device=device),\n                torch.tensor(r, dtype=torch.float32, device=device),\n                torch.tensor(np.array(ns), dtype=torch.float32, device=device),\n                torch.tensor(d, dtype=torch.float32, device=device))\n\ndef save_checkpoint(episode, steps, policy_net, optimizer, memory):\n    checkpoint = {\n        'episode': episode,\n        'steps': steps,\n        'model_state': policy_net.state_dict(),\n        'optimizer_state': optimizer.state_dict(),\n        # 'memory': list(memory.buffer) # Uncomment to save replay buffer (large!)\n    }\n    torch.save(checkpoint, CHECKPOINT_SAVE)\n    print(f\"üíæ Checkpoint saved at ep {episode}\")\n\ndef load_checkpoint(policy_net, optimizer, target_net, memory):\n    if os.path.exists(CHECKPOINT_FILE):\n        print(\"üîÑ Loading checkpoint...\")\n        checkpoint = torch.load(CHECKPOINT_FILE, map_location=device, weights_only=False)\n        policy_net.load_state_dict(checkpoint['model_state'])\n        optimizer.load_state_dict(checkpoint['optimizer_state'])\n        target_net.load_state_dict(policy_net.state_dict())\n        return checkpoint['episode'], checkpoint['steps']\n    return 0, 0\n\n# --- MAIN TRAINING LOOP ---\nif __name__ == \"__main__\":\n    # Init Objects\n    policy_net = DQN().to(device)\n    target_net = DQN().to(device)\n    target_net.load_state_dict(policy_net.state_dict())\n    target_net.eval()\n\n    optimizer = optim.Adam(policy_net.parameters(), lr=LR)\n    memory = ReplayBuffer(MEMORY_SIZE)\n    criterion = nn.HuberLoss()\n    augmenter = DataAugmenter() # New Augmenter Class\n    \n    # Load Weights\n    start_episode, total_steps = load_checkpoint(policy_net, optimizer, target_net, memory)\n    \n    # Init Vectorized Env (Rust)\n    vec_env = threes_rs.ThreesVecEnv(NUM_ENVS)\n    \n    # Tracking Metrics\n    WINDOW_SIZE = 100\n    metrics = {\n        'r': deque(maxlen=WINDOW_SIZE), # Rewards\n        'm': deque(maxlen=WINDOW_SIZE), # Moves\n        'l': deque(maxlen=WINDOW_SIZE), # Loss\n        'q': deque(maxlen=WINDOW_SIZE), # Q-Values\n        't': deque(maxlen=WINDOW_SIZE)  # Max Tiles\n    }\n    \n    # Per-Env Accumulators\n    env_curr = {\n        'moves': np.zeros(NUM_ENVS),\n        'rewards': np.zeros(NUM_ENVS),\n        'max_tile': np.zeros(NUM_ENVS)\n    }\n\n    print(f\"üöÄ Starting VECTORIZED training with {NUM_ENVS} envs...\")\n    start_time = time.time()\n\n    # Initial Observation\n    raw_boards = vec_env.reset()\n    hint_sets = vec_env.get_hint_sets()\n    states = prepare_state_batch(raw_boards, hint_sets)\n    episode_count = start_episode\n\n    while True:\n        total_steps += NUM_ENVS\n        eps = 0.05 + (0.95) * math.exp(-1. * total_steps / EPS_DECAY)\n        \n        # 1. SELECT ACTION (Batch)\n        valid_masks = vec_env.valid_moves_batch()\n        valid_masks_t = torch.tensor(valid_masks, device=device, dtype=torch.bool)\n        \n        with torch.no_grad():\n            states_t = torch.tensor(states, dtype=torch.float32, device=device)\n            q_values_all = policy_net(states_t)\n            q_values_all[~valid_masks_t] = -float('inf') # Action Masking\n            \n            if random.random() > eps:\n                actions = q_values_all.argmax(dim=1).cpu().numpy().tolist()\n            else:\n                # Random valid move sampling\n                actions = []\n                for mask in valid_masks:\n                    valid_indices = [j for j, v in enumerate(mask) if v]\n                    actions.append(random.choice(valid_indices) if valid_indices else 0)\n\n        # 2. STEP (Parallel Rust Env)\n        next_boards, rewards, dones = vec_env.step(actions)\n        next_hint_sets = vec_env.get_hint_sets()\n        next_states = prepare_state_batch(next_boards, next_hint_sets)\n        \n        # 3. AUGMENT & STORE\n        # Generate 8x data per step\n        aug_batch = augmenter.augment_batch(states, actions, rewards, next_states, dones)\n        memory.push_batch(*aug_batch)\n        \n        states = next_states\n        \n        # 4. LOGGING & METRICS\n        for i in range(NUM_ENVS):\n            env_curr['moves'][i] += 1\n            env_curr['rewards'][i] += rewards[i]\n            env_curr['max_tile'][i] = max(env_curr['max_tile'][i], max(next_boards[i]))\n            \n            if dones[i]:\n                episode_count += 1\n                metrics['r'].append(env_curr['rewards'][i])\n                metrics['m'].append(env_curr['moves'][i])\n                metrics['t'].append(env_curr['max_tile'][i])\n                \n                # Reset Env Stats\n                env_curr['moves'][i] = 0\n                env_curr['rewards'][i] = 0\n                env_curr['max_tile'][i] = 0\n                \n                # Console Log\n                if episode_count % 100 == 0:\n                    elapsed = time.time() - start_time\n                    fps = total_steps / elapsed\n                    print(f\"Ep {episode_count:6d} | Steps: {total_steps:9d} | \"\n                          f\"Avg R: {np.mean(metrics['r']):6.2f} | \"\n                          f\"Moves: {np.mean(metrics['m']):4.1f} | \"\n                          f\"MaxTile: {np.mean(metrics['t']):4.0f} | \"\n                          f\"Loss: {np.mean(metrics['l']) if metrics['l'] else 0:.4f} | \"\n                          f\"Q: {np.mean(metrics['q']) if metrics['q'] else 0:.2f} | \"\n                          f\"FPS: {fps:.1f}\")\n                \n                # Save Checkpoint\n                if episode_count % 5000 == 0:\n                    save_checkpoint(episode_count, total_steps, policy_net, optimizer, memory)\n\n        # 5. TRAIN (Intensive)\n        if len(memory.buffer) >= BATCH_SIZE:\n             # Train 8 times per step to match data generation speed\n            for _ in range(8):\n                transitions = memory.sample(BATCH_SIZE)\n                b_s, b_a, b_r, b_ns, b_d = transitions\n                \n                q_eval = policy_net(b_s).gather(1, b_a.unsqueeze(1)).squeeze(1)\n                \n                with torch.no_grad():\n                    next_actions = policy_net(b_ns).argmax(1).unsqueeze(1)\n                    next_q = target_net(b_ns).gather(1, next_actions).squeeze(1)\n                    next_q[b_d.bool()] = 0.0\n                    expected_q = b_r + (GAMMA * next_q)\n                \n                loss = criterion(q_eval, expected_q)\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n                \n                metrics['l'].append(loss.item())\n                metrics['q'].append(q_eval.mean().item())\n\n        # Update Target Net\n        if total_steps % TARGET_UPDATE < NUM_ENVS:\n            target_net.load_state_dict(policy_net.state_dict())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-30T02:10:08.557945Z","iopub.execute_input":"2026-01-30T02:10:08.558263Z","iopub.status.idle":"2026-01-30T02:27:58.621380Z","shell.execute_reply.started":"2026-01-30T02:10:08.558224Z","shell.execute_reply":"2026-01-30T02:27:58.620410Z"}},"outputs":[],"execution_count":null}]}